{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f76ab6",
   "metadata": {},
   "source": [
    "## 1. Review Bag of Word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c9a01c4",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:880/1*hLvya7MXjsSc3NS2SoLMEg.png\" width=\"500\" height=\"150\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace98365",
   "metadata": {},
   "source": [
    "## 2. Review Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb4fa8",
   "metadata": {},
   "source": [
    "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2023/03/word_embedding.drawio.png\" width=\"500\" height=\"150\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf2c06",
   "metadata": {},
   "source": [
    "## 3. Review CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e99b7",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-eu-west-1.amazonaws.com/ppreviews-plos-725668748/22001760/preview.jpg\" width=\"500\" height=\"150\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b33a9",
   "metadata": {},
   "source": [
    "<img src=\"https://aegis4048.github.io/images/featured_images/skip-gram.png\" width=\"700\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922989b",
   "metadata": {},
   "source": [
    "## 4. Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d4a71",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/t7slV.png\" width=\"800\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d6351",
   "metadata": {},
   "source": [
    "The = [0.7, 0.3, 0.8]\n",
    "Cat = [0.9, 0.6, 0.3]\n",
    "Sat = [0.2, 0.3, 0.6]\n",
    "On = [0,3, 0.8, 0.2]\n",
    "Chair = [0,3, 0.2, 0.4]\n",
    "Paragraph ID = [0.6, 0.5, 0.6]\n",
    "\n",
    "Word2Vec di Rata2:\n",
    "The Cat Sat On Chair = [... ... ...]\n",
    "\n",
    "Doc2vec:\n",
    "The Cat Sat On Chair = Paragraph ID = [0.6, 0.5, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68623c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4dd0d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/muhamadsyukron/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9adfe50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57ed26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Manchester is blue\",\n",
    "        \"Manchester City will beat Manchester United\",\n",
    "        \"Manchester City has Halland\",\n",
    "        \"Manchester United will never win trophy again\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0ebe554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['manchester', 'is', 'blue'], tags=['0']),\n",
       " TaggedDocument(words=['manchester', 'city', 'will', 'beat', 'manchester', 'united'], tags=['1']),\n",
       " TaggedDocument(words=['manchester', 'city', 'has', 'halland'], tags=['2']),\n",
       " TaggedDocument(words=['manchester', 'united', 'will', 'never', 'win', 'trophy', 'again'], tags=['3'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f16d12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vector_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vector_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm=1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edcd98b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_sentence_vector [ 0.00526825 -0.01841337  0.01661368  0.01594489 -0.00625326 -0.02545988\n",
      " -0.02539287  0.01565343  0.01804327  0.00381565  0.00938473  0.00335793\n",
      " -0.01443724 -0.00039268 -0.00059483 -0.00128179  0.001208   -0.02573406\n",
      "  0.00439556  0.00372214]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"Manchester City win UCL\".lower())\n",
    "new_sentence_vector = model.infer_vector(test_data)\n",
    "print(\"new_sentence_vector\", new_sentence_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6027ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 1, Similarity: 0.23150844871997833, Sentence: Manchester City will beat Manchester United\n",
      "Document ID: 2, Similarity: 0.17332130670547485, Sentence: Manchester City has Halland\n",
      "Document ID: 3, Similarity: 0.14147937297821045, Sentence: Manchester United will never win trophy again\n",
      "Document ID: 0, Similarity: 0.10534681379795074, Sentence: Manchester is blue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/l0nw77zs49ngzv8jdxrtpfbh0000gn/T/ipykernel_1229/126530296.py:16: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_documents = model.docvecs.most_similar([new_sentence_vector])\n"
     ]
    }
   ],
   "source": [
    "# Load the trained Doc2Vec model\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# Tokenize and preprocess the new sentence\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "new_sentence = \"Manchester City win UCL\"\n",
    "new_sentence_tokens = word_tokenize(new_sentence.lower())\n",
    "\n",
    "# Infer the vector for the new sentence\n",
    "new_sentence_vector = model.infer_vector(new_sentence_tokens)\n",
    "\n",
    "# Find the most similar document (sentence) in the corpus\n",
    "similar_documents = model.docvecs.most_similar([new_sentence_vector])\n",
    "\n",
    "# Print the most similar documents\n",
    "for doc_id, similarity in similar_documents:\n",
    "    print(f\"Document ID: {doc_id}, Similarity: {similarity}, Sentence: {data[int(doc_id)]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d4e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
