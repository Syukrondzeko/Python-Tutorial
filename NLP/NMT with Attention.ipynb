{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPHsQtD8yxfos02a/GFiHko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import re\n","import tensorflow as tf\n","import os\n","import unicodedata"],"metadata":{"id":"xCemVnKlu1PL","executionInfo":{"status":"ok","timestamp":1698113856808,"user_tz":-660,"elapsed":2541,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["tf.random.set_seed(6789)\n","np.random.seed(6789)"],"metadata":{"id":"7JcqY1ygvTAT","executionInfo":{"status":"ok","timestamp":1698113856808,"user_tz":-660,"elapsed":4,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def preprocess_sentence(sent):\n","    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) if unicodedata.category(c) != \"Mn\"])\n","    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n","    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","    sent = re.sub(r\"\\s+\", \" \", sent)\n","    sent = sent.lower()\n","    return sent"],"metadata":{"id":"bKSTI9h0vR7E","executionInfo":{"status":"ok","timestamp":1698113856809,"user_tz":-660,"elapsed":4,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def read_data(num_sent_pairs =20000):\n","    en_sents, fr_sents_in, fr_sents_out = [], [], []\n","    local_file = \"/content/fra.txt\"\n","    with open(local_file, \"r\") as fin:\n","        for i, line in enumerate(fin):\n","            en_sent, fr_sent, _ = line.strip().split('\\t')\n","            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n","            fr_sent = preprocess_sentence(fr_sent)\n","            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n","            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n","            en_sents.append(en_sent)\n","            fr_sents_in.append(fr_sent_in)\n","            fr_sents_out.append(fr_sent_out)\n","            if i >= num_sent_pairs - 1:\n","                break\n","    return en_sents, fr_sents_in, fr_sents_out"],"metadata":{"id":"uXSNX-fNvQVO","executionInfo":{"status":"ok","timestamp":1698114036271,"user_tz":-660,"elapsed":4,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["NUM_SENT_PAIRS = 1000\n","sents_en, sents_fr_in, sents_fr_out = read_data(NUM_SENT_PAIRS)"],"metadata":{"id":"ztI2So9vvOqt","executionInfo":{"status":"ok","timestamp":1698114189222,"user_tz":-660,"elapsed":699,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["sents_en[0:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDcf-wlFfkyW","executionInfo":{"status":"ok","timestamp":1698114204635,"user_tz":-660,"elapsed":8,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}},"outputId":"cbdb759f-7cd1-4ecf-87ca-d0fa72c1bfe0"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['go', '.'], ['hi', '.'], ['hi', '.'], ['run', '!'], ['run', '!']]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n","tokenizer_en.fit_on_texts(sents_en)\n","data_en = tokenizer_en.texts_to_sequences(sents_en)\n","data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\")\n","\n","tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n","tokenizer_fr.fit_on_texts(sents_fr_in)\n","tokenizer_fr.fit_on_texts(sents_fr_out)\n","\n","data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n","data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\n","data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n","data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")"],"metadata":{"id":"aNY8vYm3vL_5","executionInfo":{"status":"ok","timestamp":1698114212139,"user_tz":-660,"elapsed":5,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["data_en[0:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8H5Ji_byfpWE","executionInfo":{"status":"ok","timestamp":1698114223916,"user_tz":-660,"elapsed":7,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}},"outputId":"29bf0257-a5f3-483e-c918-1ddc2591a11b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  7,   1,   0,   0,   0],\n","       [130,   1,   0,   0,   0],\n","       [130,   1,   0,   0,   0],\n","       [ 75,   3,   0,   0,   0],\n","       [ 75,   3,   0,   0,   0]], dtype=int32)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["vocab_size_en = len(tokenizer_en.word_index)\n","vocab_size_fr = len(tokenizer_fr.word_index)\n","word2idx_en = tokenizer_en.word_index\n","idx2word_en = {v:k for k, v in word2idx_en.items()}\n","word2idx_fr = tokenizer_fr.word_index\n","idx2word_fr = {v:k for k, v in word2idx_fr.items()}\n","print(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(vocab_size_en, vocab_size_fr))\n","maxlen_en = data_en.shape[1]\n","maxlen_fr = data_fr_out.shape[1]\n","print(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x9wSWoDdvKSi","executionInfo":{"status":"ok","timestamp":1698114317996,"user_tz":-660,"elapsed":578,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}},"outputId":"0f501162-957e-4ed4-bc2a-299da209ab1e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab size (en): 380, vocab size (fr): 709\n","seqlen (en): 5, (fr): 10\n"]}]},{"cell_type":"code","source":["batch_size = 64\n","dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n","dataset = dataset.shuffle(10000)\n","test_size = NUM_SENT_PAIRS // 4\n","test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n","train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"],"metadata":{"id":"DHsK5scNvknH","executionInfo":{"status":"ok","timestamp":1698114355875,"user_tz":-660,"elapsed":683,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"vctpFR69t8f2","executionInfo":{"status":"ok","timestamp":1698114542559,"user_tz":-660,"elapsed":505,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"outputs":[],"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, num_timesteps, encoder_dim, **kwargs):\n","        super(Encoder, self).__init__(**kwargs)\n","        self.encoder_dim = encoder_dim\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_timesteps)\n","        self.rnn = tf.keras.layers.GRU(encoder_dim, return_sequences=True, return_state=True)\n","\n","    def call(self, x, state):\n","        x = self.embedding(x)\n","        x, state = self.rnn(x, initial_state=state)\n","        return x, state\n","\n","    def init_state(self, batch_size):\n","        return tf.zeros((batch_size, self.encoder_dim))"]},{"cell_type":"code","source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, num_units, phase= 'training'):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(num_units)\n","        self.W2 = tf.keras.layers.Dense(num_units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, d_state, e_states):\n","        # d_sate is the decoder state at time step j of decoder RNN\n","        # d_state.shape: [batch_size, decoder_dim]\n","        # e_states are encoder states at every timestes\n","        # e_states.shape: [batch_size, num_timesteps, encoder_dim]\n","        # add time axis to query: [batch_size, 1, decoder_dim]\n","        d_state_extend = tf.expand_dims(d_state, axis=1)\n","        # compute score [batch_size, num_timesteps, 1]\n","        attention_score = self.V(tf.keras.activations.tanh(self.W1(e_states) + self.W2(d_state_extend)))\n","        # compute weight [batch_size, num_timesteps, 1]\n","        attention_weight = tf.nn.softmax(attention_score, axis=1)\n","        # compute context [batch_size, decoder_dim]\n","        context = tf.reduce_sum(tf.linalg.matmul(tf.transpose(attention_weight, perm=[0,2,1]), e_states), axis=1)\n","        return context, attention_weight"],"metadata":{"id":"vtJdArGjuRiV","executionInfo":{"status":"ok","timestamp":1698115867154,"user_tz":-660,"elapsed":6,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Note:\n","\n","1. tf.transpose(attention_weight, perm=[0,2,1]) changes the shape of attention_weight from [batch_size, num_timesteps, 1] to [batch_size, 1, num_timesteps]. This is done because you want to perform a batched matrix multiplication next, and the dimensions must be aligned properly for this operation.\n","\n","2. tf.linalg.matmul(..., e_states) performs a batched matrix multiplication between the transposed attention_weight and e_states. Given that attention_weight is now of shape [batch_size, 1, num_timesteps] and e_states is of shape [batch_size, num_timesteps, encoder_dim], the result of this multiplication is a tensor of shape [batch_size, 1, encoder_dim]. Each element in the resulting tensor is the weighted sum of the encoder states, with the weights specified by attention_weight. In other words, it's a sum of the encoder states, but where each state is scaled by how much attention the decoder is paying to that particular state."],"metadata":{"id":"4n-L1HGX1vdg"}},{"cell_type":"code","source":["class LuongAttention(tf.keras.layers.Layer):\n","    def __init__(self, num_units):\n","        super(LuongAttention, self).__init__()\n","        self.W = tf.keras.layers.Dense(num_units)\n","\n","    def call(self, d_state, e_states):\n","        # d_sate is the decoder state at time step j of decoder RNN\n","        # d_state.shape: [batch_size, decoder_dim]\n","        # e_states are encoder states at every timestep\n","        # e_states.shape: [batch_size, num_timesteps, encoder_dim]\n","        # add time axis to query: [batch_size, 1, decoder_dim]\n","        d_state_extend = tf.expand_dims(d_state, axis=1)\n","        # compute score [batch_size, num_timesteps, 1]\n","        attention_score = tf.linalg.matmul(self.W(e_states), tf.transpose(d_state_extend, perm=(0,2,1)))\n","        # compute softmax attention_weight\n","        attention_weight = tf.nn.softmax(attention_score, axis=1)\n","        # compute attended output\n","        context = tf.linalg.matmul(tf.transpose(attention_weight, perm=[0,2,1]), e_states)\n","        context = tf.squeeze(context, axis=1) # The tf.squeeze function in TensorFlow is used to remove dimensions of size 1 from a tensor\n","        return context, attention_weight"],"metadata":{"id":"53Oxq38ovCAl","executionInfo":{"status":"ok","timestamp":1698115867154,"user_tz":-660,"elapsed":4,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["1. e_states: This tensor contains the encoder states and has a shape of [batch_size, num_timesteps, encoder_dim].\n","\n","2. self.W: This is a dense layer that projects the encoder states down to a new dimension, which is num_units. When the e_states tensor is passed through this layer, the resulting tensor has a shape of [batch_size, num_timesteps, num_units].\n","\n","3. d_state: This tensor is the decoder state and has a shape of [batch_size, decoder_dim].\n","\n","4. d_state_extend: By using tf.expand_dims(d_state, axis=1), a time axis is added to the decoder state, resulting in a shape of [batch_size, 1, decoder_dim].\n","\n","5. tf.transpose(d_state_extend, perm=(0,2,1)): This operation transposes the d_state_extend tensor, resulting in a shape of [batch_size, decoder_dim, 1].\n","\n","6. attention_score: The tf.linalg.matmul(self.W(e_states), tf.transpose(d_state_extend, perm=(0,2,1))) operation performs a batched matrix multiplication between the projected encoder states and the transposed extended decoder state. The shapes involved in this operation are:\n","\n","7. self.W(e_states): [batch_size, num_timesteps, num_units]\n","tf.transpose(d_state_extend, perm=(0,2,1)): [batch_size, decoder_dim, 1]\n","Assuming that decoder_dim is equal to num_units (i.e., the decoder's hidden state size is the same as the projection size of the encoder states), the resulting attention_score tensor will have a shape of [batch_size, num_timesteps, 1]. This tensor contains the raw attention scores for each encoder state, which are then passed through a softmax layer to produce the normalized attention_weight.\n","\n","8. If decoder_dim is not equal to num_units, the self.W layer won't be able to directly compute the scores because the inner dimensions won't match for matrix multiplication. In this case, the decoder states should also be projected to a dimension of num_units, or the num_units should be set equal to decoder_dim when initializing the LuongAttention layer.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"FHyCpAW82Sxx"}},{"cell_type":"code","source":["batch_size = 64\n","num_timesteps = 100\n","num_units = 200\n","d_state = np.random.random(size=(batch_size, num_units)).astype(np.float32)\n","e_states = np.random.random(size=(batch_size, num_timesteps, num_units)).astype(np.float32)\n","# check out dimensions for Bahdanau attention\n","b_attn = BahdanauAttention(num_units)\n","context, attention_weight = b_attn(d_state, e_states)\n","print(\"Bahdanau: context.shape:\", context.shape, \"attention_weight.shape:\", attention_weight.shape)\n","# check out dimensions for Luong attention\n","l_attn = LuongAttention(num_units)\n","context, attention_weights = l_attn(d_state, e_states)\n","print(\"Luong: context.shape:\", context.shape, \"attention_weight:\", attention_weight.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QnssvFNuTfm","executionInfo":{"status":"ok","timestamp":1697967397852,"user_tz":-660,"elapsed":5,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}},"outputId":"c2dfd361-cda0-412f-c1d8-e5f30c0c956a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bahdanau: context.shape: (64, 200) attention_weight.shape: (64, 100, 1)\n","Luong: context.shape: (64, 200) attention_weight: (64, 100, 1)\n"]}]},{"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, num_timesteps, decoder_dim, attention_type= 'Bahdanau', **kwargs):\n","        super(Decoder, self).__init__(**kwargs)\n","        self.decoder_dim = decoder_dim\n","        if attention_type == 'Bahdanau':\n","            self.attention = BahdanauAttention(decoder_dim)\n","        else:\n","            self.attention = LuongAttention(decoder_dim)\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_timesteps)\n","        self.rnn = tf.keras.layers.GRU(decoder_dim, return_sequences=False, return_state=True)\n","        self.Wc = tf.keras.layers.Dense(decoder_dim, activation=\"tanh\")\n","        self.Ws = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, x, state, encoder_out):\n","        x = self.embedding(x)\n","        h, state = self.rnn(x, state)\n","        context, attention_weight = self.attention(h, encoder_out)\n","        h= tf.concat([h, context], axis=1)\n","        h = self.Wc(h)\n","        logits = self.Ws(h)\n","        return logits, state,attention_weight"],"metadata":{"id":"OZ2gX-GQuVvW","executionInfo":{"status":"ok","timestamp":1698116110882,"user_tz":-660,"elapsed":622,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["embedding_dim = 256\n","encoder_dim, decoder_dim = 100, 100\n","encoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\n","decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim, 'Bahdanau')"],"metadata":{"id":"lXRSXb7DuX4U","executionInfo":{"status":"ok","timestamp":1698116216601,"user_tz":-660,"elapsed":3,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def loss_fn(ytrue, ypred):\n","    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n","    mask = tf.cast(mask, dtype=tf.int64)\n","    loss = scce(ytrue, ypred, sample_weight=mask)\n","    return loss"],"metadata":{"id":"xlDTuOYpucRj","executionInfo":{"status":"ok","timestamp":1698116262521,"user_tz":-660,"elapsed":3,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam()\n","\n","@tf.function\n","def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n","    with tf.GradientTape() as tape:\n","        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n","        decoder_state = encoder_state\n","        loss = 0\n","        for t in range(decoder_out.shape[1]):\n","            decoder_in_t = decoder_in[:, t]  # the t-th word of sentences in current batch\n","            decoder_in_t = tf.reshape(decoder_in_t, [-1,1])\n","            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t, decoder_state, encoder_out)\n","            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","    return loss / decoder_out.shape[1]"],"metadata":{"id":"c8E-9XoRue-c","executionInfo":{"status":"ok","timestamp":1698116733990,"user_tz":-660,"elapsed":5,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def predict(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr):\n","    random_id = np.random.choice(len(sents_en))\n","    print(\"input : \", \" \".join(sents_en[random_id]))\n","    print(\"label : \", \" \".join(sents_fr_out[random_id]))\n","    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n","    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n","    encoder_state = encoder.init_state(1)\n","    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n","    decoder_state = encoder_state\n","    decoder_in = tf.expand_dims(tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\n","    pred_sent_fr = []\n","    decoding_step = 0\n","    while decoding_step < maxlen_fr:\n","        decoder_pred, decoder_state,_ = decoder(decoder_in, decoder_state, encoder_out)\n","        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n","        pred_word = idx2word_fr[decoder_pred.numpy()[0]]\n","        pred_sent_fr.append(pred_word)\n","        if pred_word == \"EOS\":\n","            break\n","        decoder_in = tf.reshape(decoder_pred, [1,-1])\n","        decoding_step += 1\n","    print(\"predicted: \", \" \".join(pred_sent_fr))"],"metadata":{"id":"xjJAmf5ouhJA","executionInfo":{"status":"ok","timestamp":1698116734490,"user_tz":-660,"elapsed":17,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def train_all():\n","    num_epochs = 25\n","    for e in range(num_epochs):\n","        encoder_state = encoder.init_state(batch_size)\n","        for batch, data in enumerate(train_dataset):\n","            encoder_in, decoder_in, decoder_out = data\n","            # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n","            loss = train_step(encoder_in, decoder_in, decoder_out, encoder_state)\n","            # print(\"Batch {}: loss = {}\".format(batch, loss))\n","        print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n","        predict(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr)\n","\n","train_all()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9mVKzxTuip4","executionInfo":{"status":"ok","timestamp":1698116808472,"user_tz":-660,"elapsed":42589,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}},"outputId":"8b4ba31f-f8f2-4a1e-a0be-d4d07e1df6b3"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 3.1847\n","input :  call me .\n","label :  appelez moi ! EOS\n","predicted:  EOS\n","Epoch: 2, Loss: 2.1612\n","input :  let s see .\n","label :  voyons voir ! EOS\n","predicted:  EOS\n","Epoch: 3, Loss: 2.0937\n","input :  it stinks .\n","label :  ca pue . EOS\n","predicted:  EOS\n","Epoch: 4, Loss: 1.9888\n","input :  open up .\n","label :  ouvre moi ! EOS\n","predicted:  je . EOS\n","Epoch: 5, Loss: 1.9875\n","input :  i m right .\n","label :  j ai raison . EOS\n","predicted:  je je EOS\n","Epoch: 6, Loss: 1.7733\n","input :  they fell .\n","label :  ils sont tombes . EOS\n","predicted:  je je . EOS\n","Epoch: 7, Loss: 1.6487\n","input :  see you !\n","label :  a la prochaine ! EOS\n","predicted:  je je . EOS\n","Epoch: 8, Loss: 1.8946\n","input :  i retired .\n","label :  j ai pris ma retraite . EOS\n","predicted:  je suis je . EOS\n","Epoch: 9, Loss: 1.7535\n","input :  it was ok .\n","label :  c etait ok . EOS\n","predicted:  je suis je . EOS\n","Epoch: 10, Loss: 1.5737\n","input :  i m home .\n","label :  je suis chez moi . EOS\n","predicted:  je suis je . EOS\n","Epoch: 11, Loss: 1.5606\n","input :  i phoned .\n","label :  j ai telephone . EOS\n","predicted:  je suis je . EOS\n","Epoch: 12, Loss: 1.5022\n","input :  i m going .\n","label :  je pars . EOS\n","predicted:  je suis je . EOS\n","Epoch: 13, Loss: 1.4003\n","input :  i failed .\n","label :  j ai echoue . EOS\n","predicted:  je suis je . EOS\n","Epoch: 14, Loss: 1.5272\n","input :  i helped .\n","label :  j ai aide . EOS\n","predicted:  je suis je . EOS\n","Epoch: 15, Loss: 1.3934\n","input :  don t die .\n","label :  ne meurs pas ! EOS\n","predicted:  je suis je suis je . EOS\n","Epoch: 16, Loss: 1.4261\n","input :  i m wet .\n","label :  je suis mouille . EOS\n","predicted:  je suis je ! EOS\n","Epoch: 17, Loss: 1.4373\n","input :  beats me .\n","label :  aucune idee . EOS\n","predicted:  je suis je ! EOS\n","Epoch: 18, Loss: 1.3289\n","input :  we lost .\n","label :  nous avons ete battues . EOS\n","predicted:  je suis je ! EOS\n","Epoch: 19, Loss: 1.3153\n","input :  i m shy .\n","label :  je suis timide . EOS\n","predicted:  je suis je . EOS\n","Epoch: 20, Loss: 1.1889\n","input :  head west .\n","label :  dirigez vous vers l ouest . EOS\n","predicted:  venez moi ! EOS\n","Epoch: 21, Loss: 1.2975\n","input :  may i go ?\n","label :  puis je m y rendre ? EOS\n","predicted:  c suis en ai en en ai en . EOS\n","Epoch: 22, Loss: 1.2706\n","input :  i m bald .\n","label :  je suis chauve . EOS\n","predicted:  je suis en . EOS\n","Epoch: 23, Loss: 1.2719\n","input :  she walks .\n","label :  elle marche . EOS\n","predicted:  c est la la . EOS\n","Epoch: 24, Loss: 1.1654\n","input :  i m ready !\n","label :  je suis prete ! EOS\n","predicted:  je suis gagne . EOS\n","Epoch: 25, Loss: 1.1400\n","input :  how weird !\n","label :  comme c est bizarre ! EOS\n","predicted:  c est la la ! EOS\n"]}]},{"cell_type":"code","source":["embedding_dim = 256\n","encoder_dim, decoder_dim = 100, 100\n","encoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\n","decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim, 'Luong')\n","optimizer = tf.keras.optimizers.Adam()\n","\n","@tf.function\n","def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n","    with tf.GradientTape() as tape:\n","        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n","        decoder_state = encoder_state\n","        loss = 0\n","        for t in range(decoder_out.shape[1]):\n","            decoder_in_t = decoder_in[:, t]  # the t-th word of sentences in current batch\n","            decoder_in_t = tf.reshape(decoder_in_t, [-1,1])\n","            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t, decoder_state, encoder_out)\n","            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","    return loss / decoder_out.shape[1]\n","\n","train_all()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cpWckpuuned","executionInfo":{"status":"ok","timestamp":1697967582194,"user_tz":-660,"elapsed":41480,"user":{"displayName":"Manusia Setengah Chi Kuadrat","userId":"06568609713576783368"}},"outputId":"2413d7bf-79b6-439c-ce9a-81bc1414ebde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 2.9254\n","input :  i m right .\n","label :  j ai raison . EOS\n","predicted:  EOS\n","Epoch: 2, Loss: 2.2139\n","input :  back off .\n","label :  recule ! EOS\n","predicted:  EOS\n","Epoch: 3, Loss: 1.9798\n","input :  i m well .\n","label :  je me porte bien . EOS\n","predicted:  EOS\n","Epoch: 4, Loss: 2.0428\n","input :  jump .\n","label :  saute . EOS\n","predicted:  EOS\n","Epoch: 5, Loss: 2.0269\n","input :  it s hers .\n","label :  c est le sien . EOS\n","predicted:  EOS\n","Epoch: 6, Loss: 1.9922\n","input :  it poured .\n","label :  il pleuvait a verse . EOS\n","predicted:  . EOS\n","Epoch: 7, Loss: 1.9475\n","input :  don t die .\n","label :  ne mourez pas ! EOS\n","predicted:  je je EOS\n","Epoch: 8, Loss: 1.6496\n","input :  i m angry .\n","label :  je suis enerve . EOS\n","predicted:  je . EOS\n","Epoch: 9, Loss: 1.7760\n","input :  i relaxed .\n","label :  je me suis detendu . EOS\n","predicted:  je je . EOS\n","Epoch: 10, Loss: 1.7943\n","input :  he s sexy .\n","label :  il est sexy . EOS\n","predicted:  je je . EOS\n","Epoch: 11, Loss: 1.6851\n","input :  take care !\n","label :  soyez prudente ! EOS\n","predicted:  je suis . EOS\n","Epoch: 12, Loss: 1.7208\n","input :  i m deaf .\n","label :  je suis sourde . EOS\n","predicted:  je suis suis . EOS\n","Epoch: 13, Loss: 1.5736\n","input :  shut up !\n","label :  taisez vous ! EOS\n","predicted:  je suis suis . EOS\n","Epoch: 14, Loss: 1.4764\n","input :  go slow .\n","label :  va doucement ! EOS\n","predicted:  je suis ! EOS\n","Epoch: 15, Loss: 1.6188\n","input :  i can ski .\n","label :  je sais skier . EOS\n","predicted:  je suis ! EOS\n","Epoch: 16, Loss: 1.7089\n","input :  i tried .\n","label :  j ai tente . EOS\n","predicted:  je suis suis ! EOS\n","Epoch: 17, Loss: 1.6050\n","input :  stay away .\n","label :  reste a l ecart . EOS\n","predicted:  je suis ! EOS\n","Epoch: 18, Loss: 1.4164\n","input :  what for ?\n","label :  pour quoi faire ? EOS\n","predicted:  je suis ! EOS\n","Epoch: 19, Loss: 1.4028\n","input :  come back .\n","label :  revenez ! EOS\n","predicted:  je suis ! EOS\n","Epoch: 20, Loss: 1.3165\n","input :  i agree .\n","label :  je suis du meme avis . EOS\n","predicted:  je suis suis ! EOS\n","Epoch: 21, Loss: 1.3816\n","input :  i m tom .\n","label :  je suis tom . EOS\n","predicted:  je suis suis ! EOS\n","Epoch: 22, Loss: 1.3894\n","input :  come over .\n","label :  venez ici ! EOS\n","predicted:  laissez est ! EOS\n","Epoch: 23, Loss: 1.4191\n","input :  i can run .\n","label :  je sais courir . EOS\n","predicted:  je suis est ! EOS\n","Epoch: 24, Loss: 1.2311\n","input :  wait !\n","label :  attends ! EOS\n","predicted:  laissez ! EOS\n","Epoch: 25, Loss: 1.4081\n","input :  let me in .\n","label :  laissez moi entrer . EOS\n","predicted:  c est moi ! EOS\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kVTw1ppbvehi"},"execution_count":null,"outputs":[]}]}